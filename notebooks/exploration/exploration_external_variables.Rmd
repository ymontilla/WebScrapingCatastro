---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.5.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# %load_ext dotenv
# %reload_ext dotenv
# %dotenv
```

```{python}
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

import folium
from folium.plugins import HeatMap
from shapely.geometry import MultiPoint
```

```{python}
import os
import sys

from os.path import dirname
UTILS_PATH=os.environ.get('REPO') + "/notebooks/commons"
sys.path.append(dirname(UTILS_PATH))

from commons import check_args_variables, gen_paths_variables, coordinates_bounds
```

```{python}

```

```{python}
if __name__ == "__main__":
    source = "PLACES"
    city = "manizales"
    
    base_path = os.environ.get('REPO')
    
    source_path = base_path + gen_paths_variables(source, city)
```

```{python}
MSG = """
Hay {} publicaciones de {} en total
"""

external_variables = pd.read_parquet(source_path)
print(MSG.format(external_variables.shape[0], city))
external_variables.head()
```

```{python}

```

```{python}
from sklearn.cluster import DBSCAN
```

```{python}
coords = external_variables[['lat','lon']].copy()
coords.loc[:, "lat"] = coords["lat"].apply(float)
coords.loc[:, "lon"] = coords["lon"].apply(float)
coords = coords.values

kms_per_radian = 6371.0088
epsilon = 0.1 / kms_per_radian

min_samples = 20
```

```{python}
db = DBSCAN(eps=epsilon, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))
```

```{python}
centroids = []
clusters_size = len(db.labels_)

for n in range(clusters_size):
    cluster = external_variables[['lat','lon']][db.labels_ == n].copy()
    if len(cluster) > 0:
        cluster.loc[:, "cluster_id"] = n
        
        multi_point = MultiPoint(cluster.values)
        cluster.loc[:, "cluster_latitude"] = multi_point.centroid.x
        cluster.loc[:, "cluster_longitude"] = multi_point.centroid.y
        
    centroids.append(cluster)
        
clusters_df = pd.concat(centroids)
clusters_df = clusters_df.reset_index(drop=True)
clusters_df.head()
```

```{python}
clusters_coordinates = clusters_df[["cluster_id", "lat", "lon"]].groupby(["lat", "lon"]).count()

coordinates = list(zip(*clusters_coordinates.index))
clusters_coordinates.loc[:, "lat"] = coordinates[0]
clusters_coordinates.loc[:, "lon"] = coordinates[1]

clusters_coordinates.loc[:, "count"] = clusters_coordinates["cluster_id"].apply(lambda e: float(e*100))

clusters_coordinates = clusters_coordinates.reset_index(drop=True)

clusters_coordinates.head()
```

```{python}
folium_hmap = folium.Map(location=[
    np.mean([coordinates_bounds[city]["lat"]["lower"], coordinates_bounds[city]["lat"]["upper"]]),
    np.mean([coordinates_bounds[city]["lon"]["lower"], coordinates_bounds[city]["lon"]["upper"]])
], zoom_start=13, tiles="OpenStreetMap")

max_amount = clusters_coordinates['count'].max()

hm_wide = HeatMap( 
    clusters_coordinates[["lat", "lon", "count"]],
    min_opacity=0.2,
    max_val=max_amount,
    radius=8, blur=6, 
    max_zoom=15, 
    gradient={.1:'yellow', .5: 'orange',  1: 'red'}
)

folium_hmap.add_child(hm_wide)
```

```{python}

```
